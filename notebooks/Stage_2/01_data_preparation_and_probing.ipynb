{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 12544 sentences.\n",
      "Example:\n",
      "{'words': ['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.'], 'pos_tags': ['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']}\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse_incr\n",
    "\n",
    "# Path to your data\n",
    "train_file = \"../../data/ud_ewt/en_ewt-ud-train.conllu\"\n",
    "\n",
    "# Store parsed sentences\n",
    "sentences = []\n",
    "\n",
    "# Parse the .conllu file\n",
    "with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for tokenlist in parse_incr(f):\n",
    "        words = []\n",
    "        pos_tags = []\n",
    "        \n",
    "        for token in tokenlist:\n",
    "            if isinstance(token['id'], int):  # Ignore multi-word tokens (e.g., can't)\n",
    "                words.append(token['form'])\n",
    "                pos_tags.append(token['upostag'])\n",
    "        \n",
    "        sentences.append({\n",
    "            \"words\": words,\n",
    "            \"pos_tags\": pos_tags\n",
    "        })\n",
    "\n",
    "print(f\"Parsed {len(sentences)} sentences.\")\n",
    "print(\"Example:\")\n",
    "print(sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Define linguistic features as sets of Universal POS tags.\n",
    "# Each set corresponds to one binary linguistic concept we want to probe for or erase via LEACE.\n",
    "# ----------------------------------------------------------------------\n",
    "FEATURE_SETS = {\n",
    "    \"function_content\": {\"ADP\", \"AUX\", \"CCONJ\", \"DET\", \"PART\", \"PRON\", \"SCONJ\"},\n",
    "    \"noun_nonnoun\":     {\"NOUN\", \"PROPN\"},\n",
    "    \"verb_nonverb\":     {\"VERB\", \"AUX\"},\n",
    "    \"closed_open\":      {\"ADP\", \"AUX\", \"CCONJ\", \"DET\", \"PART\", \"PRON\", \"SCONJ\", \"PUNCT\", \"SYM\"}\n",
    "}\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Map a list of POS tags (e.g., from UD .conllu file) to a dictionary of\n",
    "# binary feature label lists — one per feature name.\n",
    "#\n",
    "# This structure is better for LEACE-style analysis because:\n",
    "# - Each feature can be erased independently\n",
    "# - Remaining features can be used to evaluate preservation vs. removal\n",
    "# - Easily scale this to more or different features\n",
    "# - Systematically compare how erasing one affects the others\n",
    "# ----------------------------------------------------------------------\n",
    "def get_feature_matrix(pos_tags):\n",
    "    \"\"\"\n",
    "    Convert a sequence of POS tags into a dictionary of binary feature labels.\n",
    "    Each entry in the dictionary corresponds to a binary classification target:\n",
    "    1 = belongs to the feature (e.g., is a function word), 0 = does not.\n",
    "    \n",
    "    Args:\n",
    "        pos_tags (List[str]): POS tags for each word in a sentence.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[int]]: Mapping from feature name to binary label sequence.\n",
    "    \"\"\"\n",
    "    labels = {}\n",
    "    for feature_name, pos_set in FEATURE_SETS.items():\n",
    "        labels[feature_name] = [\n",
    "            1 if pos in pos_set else 0\n",
    "            for pos in pos_tags\n",
    "        ]\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12544/12544 [00:09<00:00, 1373.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 12544 sentences.\n",
      "['[', 'This', 'killing', 'of', 'a', 'respected', 'cler', 'ic', 'will', 'be', 'ca', 'using', 'us', 't', 'rou', 'ble', 'for', 'years', 'to', 'come', '.', ']']\n",
      "Token spans per word: [[0], [1], [2], [3], [4], [5], [6, 7], [8], [9], [10, 11], [12], [13, 14, 15], [16], [17], [18], [19], [20], [21]]\n",
      "function_content labels: [0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0]\n",
      "noun_nonnoun labels: [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]\n",
      "verb_nonverb labels: [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "closed_open labels: [1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Needed for batching\n",
    "\n",
    "tokenized_sentences = []\n",
    "\n",
    "for sentence in tqdm(sentences):\n",
    "    words = sentence[\"words\"]\n",
    "    pos_tags = sentence[\"pos_tags\"]\n",
    "\n",
    "    feature_labels = get_feature_matrix(pos_tags)  # word-level labels\n",
    "    feature_names = list(feature_labels.keys())\n",
    "\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    word_to_token_positions = []  # tracks subtoken spans per word\n",
    "    word_labels = {feature: [] for feature in feature_names}  # one label per word\n",
    "\n",
    "    current_token_position = 0\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        word_tokens = tokenizer.tokenize(word)  # e.g., \"unbelievable\" → ['un', 'believable']\n",
    "        word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "\n",
    "        if not word_ids:\n",
    "            continue\n",
    "\n",
    "        # Store token span for this word\n",
    "        token_positions = list(range(current_token_position, current_token_position + len(word_ids)))\n",
    "        word_to_token_positions.append(token_positions)\n",
    "\n",
    "        # Update sequences\n",
    "        input_ids.extend(word_ids)\n",
    "        attention_mask.extend([1] * len(word_ids))\n",
    "        current_token_position += len(word_ids)\n",
    "\n",
    "        # Add word-level labels\n",
    "        for feature in feature_names:\n",
    "            word_labels[feature].append(feature_labels[feature][i])\n",
    "\n",
    "    # Store sentence\n",
    "    tokenized_sentences.append({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"word_to_token_positions\": word_to_token_positions,\n",
    "        \"word_labels\": word_labels\n",
    "    })\n",
    "\n",
    "# -------------------- Notes on design --------------------\n",
    "# - GPT-2 uses BPE → words can split into multiple tokens\n",
    "# - GPT-2 does not prioritize first/last token for meaning (unlike BERT)\n",
    "# - Averaging all subtokens = more faithful word-level embedding\n",
    "# - LEACE works best when embeddings match word-level labels\n",
    "# ----------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 12544 sentences.\n",
      "['Al', '-', 'Z', 'aman', ':', 'American', 'forces', 'killed', 'Sh', 'a', 'ikh', 'Ab', 'dullah', 'al', '-', 'An', 'i', ',', 'the', 'pre', 'acher', 'at', 'the', 'mos', 'que', 'in', 'the', 'town', 'of', 'Q', 'aim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
      "Token spans per word: [[0], [1], [2, 3], [4], [5], [6], [7], [8, 9, 10], [11, 12], [13], [14], [15, 16], [17], [18], [19, 20], [21], [22], [23, 24], [25], [26], [27], [28], [29, 30], [31], [32], [33], [34], [35], [36]]\n",
      "function_content labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
      "noun_nonnoun labels: [1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0]\n",
      "verb_nonverb labels: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "closed_open labels: [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Example output\n",
    "print(f\"Tokenized {len(tokenized_sentences)} sentences.\")\n",
    "example = tokenized_sentences[0]\n",
    "print(tokenizer.convert_ids_to_tokens(example[\"input_ids\"]))\n",
    "print(\"Token spans per word:\", example[\"word_to_token_positions\"])\n",
    "for feature in feature_names:\n",
    "    print(f\"{feature} labels:\", example[\"word_labels\"][feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenized dataset to tokenized_sentences.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Adjust the filename as needed\n",
    "save_path = \"tokenized_sentences.pkl\"\n",
    "\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(tokenized_sentences, f)\n",
    "\n",
    "print(f\"Saved tokenized dataset to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on just a few sentences first\n",
    "num_test_sentences = 3\n",
    "test_sentences = tokenized_sentences[:num_test_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings for 3 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Here's a preview of the first test sample:\n",
      "Number of layers: 13\n",
      "Number of words: 29\n",
      "First 5 POS labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load GPT-2 model\n",
    "model = GPT2Model.from_pretrained(\"gpt2\", output_hidden_states=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "def average_subtokens_per_word(hidden_states, word_to_token_positions):\n",
    "    \"\"\"Average token embeddings over each word's subtoken span\"\"\"\n",
    "    word_embeddings = []\n",
    "    for token_idxs in word_to_token_positions:\n",
    "        vectors = hidden_states[token_idxs]\n",
    "        avg_vector = vectors.mean(dim=0)\n",
    "        word_embeddings.append(avg_vector)\n",
    "    return torch.stack(word_embeddings)\n",
    "\n",
    "# Store results for testing\n",
    "test_outputs = []\n",
    "\n",
    "for example in test_sentences:\n",
    "    input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device)\n",
    "    attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        all_layers = outputs.hidden_states  # list of [1, seq_len, hidden_dim]\n",
    "\n",
    "    word_embeddings_by_layer = []\n",
    "\n",
    "    for layer_tensor in all_layers:\n",
    "        layer_tensor = layer_tensor.squeeze(0)  # shape: [seq_len, hidden_dim]\n",
    "        word_embeddings = average_subtokens_per_word(\n",
    "            layer_tensor,\n",
    "            example[\"word_to_token_positions\"]\n",
    "        )  # shape: [num_words, hidden_dim]\n",
    "        word_embeddings_by_layer.append(word_embeddings.cpu())\n",
    "\n",
    "    test_outputs.append({\n",
    "        \"embeddings_by_layer\": word_embeddings_by_layer,\n",
    "        \"word_labels\": example[\"word_labels\"]\n",
    "    })\n",
    "\n",
    "# Quick check\n",
    "print(\"✅ Done. Here's a preview of the first test sample:\")\n",
    "print(\"Number of layers:\", len(test_outputs[0][\"embeddings_by_layer\"]))\n",
    "print(\"Number of words:\", test_outputs[0][\"embeddings_by_layer\"][0].shape[0])\n",
    "print(\"First 5 POS labels:\", test_outputs[0][\"word_labels\"][\"function_content\"][:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_labels(sentence, feature_sets, verbose=True):\n",
    "    \"\"\"\n",
    "    Compare POS tags to generated binary labels for each feature.\n",
    "\n",
    "    Args:\n",
    "        sentence (dict): Contains 'words' and 'pos_tags'\n",
    "        feature_sets (dict): Mapping feature_name → set of POS tags\n",
    "        verbose (bool): If True, prints comparison\n",
    "\n",
    "    Returns:\n",
    "        bool: True if all labels match expectations\n",
    "    \"\"\"\n",
    "    words = sentence[\"words\"]\n",
    "    pos_tags = sentence[\"pos_tags\"]\n",
    "    labels = get_feature_matrix(pos_tags)\n",
    "\n",
    "    all_correct = True\n",
    "\n",
    "    # Header\n",
    "    if verbose:\n",
    "        header = f\"{'Word':<15}{'POS':<10}\"\n",
    "        for feat in labels:\n",
    "            header += f\"{feat + ' (gold/pred)':<20}\"\n",
    "        print(header)\n",
    "\n",
    "    # Row-by-row comparison\n",
    "    for i, (word, pos) in enumerate(zip(words, pos_tags)):\n",
    "        expected = {\n",
    "            feat: int(pos in feature_sets[feat])\n",
    "            for feat in feature_sets\n",
    "        }\n",
    "        generated = {\n",
    "            feat: labels[feat][i]\n",
    "            for feat in labels\n",
    "        }\n",
    "\n",
    "        row_correct = expected == generated\n",
    "        all_correct = all_correct and row_correct\n",
    "\n",
    "        if verbose:\n",
    "            row = f\"{word:<15}{pos:<10}\"\n",
    "            for feat in labels:\n",
    "                gold = expected[feat]\n",
    "                pred = generated[feat]\n",
    "                mismatch = \" ❌\" if gold != pred else \"\"\n",
    "                row += f\"{f'{gold}/{pred}':<20}{mismatch}\"\n",
    "            print(row)\n",
    "\n",
    "    return all_correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           POS       function_content (gold/pred)noun_nonnoun (gold/pred)verb_nonverb (gold/pred)closed_open (gold/pred)\n",
      "Al             PROPN     0/0                 1/1                 0/0                 0/0                 \n",
      "-              PUNCT     0/0                 0/0                 0/0                 1/1                 \n",
      "Zaman          PROPN     0/0                 1/1                 0/0                 0/0                 \n",
      ":              PUNCT     0/0                 0/0                 0/0                 1/1                 \n",
      "American       ADJ       0/0                 0/0                 0/0                 0/0                 \n",
      "forces         NOUN      0/0                 1/1                 0/0                 0/0                 \n",
      "killed         VERB      0/0                 0/0                 1/1                 0/0                 \n",
      "Shaikh         PROPN     0/0                 1/1                 0/0                 0/0                 \n",
      "Abdullah       PROPN     0/0                 1/1                 0/0                 0/0                 \n",
      "al             PROPN     0/0                 1/1                 0/0                 0/0                 \n",
      "-              PUNCT     0/0                 0/0                 0/0                 1/1                 \n",
      "Ani            PROPN     0/0                 1/1                 0/0                 0/0                 \n",
      ",              PUNCT     0/0                 0/0                 0/0                 1/1                 \n",
      "the            DET       1/1                 0/0                 0/0                 1/1                 \n",
      "preacher       NOUN      0/0                 1/1                 0/0                 0/0                 \n",
      "at             ADP       1/1                 0/0                 0/0                 1/1                 \n",
      "the            DET       1/1                 0/0                 0/0                 1/1                 \n",
      "mosque         NOUN      0/0                 1/1                 0/0                 0/0                 \n",
      "in             ADP       1/1                 0/0                 0/0                 1/1                 \n",
      "the            DET       1/1                 0/0                 0/0                 1/1                 \n",
      "town           NOUN      0/0                 1/1                 0/0                 0/0                 \n",
      "of             ADP       1/1                 0/0                 0/0                 1/1                 \n",
      "Qaim           PROPN     0/0                 1/1                 0/0                 0/0                 \n",
      ",              PUNCT     0/0                 0/0                 0/0                 1/1                 \n",
      "near           ADP       1/1                 0/0                 0/0                 1/1                 \n",
      "the            DET       1/1                 0/0                 0/0                 1/1                 \n",
      "Syrian         ADJ       0/0                 0/0                 0/0                 0/0                 \n",
      "border         NOUN      0/0                 1/1                 0/0                 0/0                 \n",
      ".              PUNCT     0/0                 0/0                 0/0                 1/1                 \n",
      "✅ All labels match.\n"
     ]
    }
   ],
   "source": [
    "# Test it on one sentence\n",
    "correct = inspect_labels(sentences[0], FEATURE_SETS)\n",
    "\n",
    "print(\"✅ All labels match.\" if correct else \"❌ Mismatches found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
