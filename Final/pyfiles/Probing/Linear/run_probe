"""
Unified script for probing, with corrected logic for LEACE evaluation
that strictly follows the "evaluate on test set" methodology from the paper,
while retaining the in-sample evaluation for the Oracle method.

This final version saves the resulting tables to .csv files.
"""
import argparse
import pickle
import numpy as np
import os
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.preprocessing import StandardScaler
from numpy.linalg import norm
from tqdm import tqdm

# =========================================================================
# 1. Helper Functions
# =========================================================================

def cosine_similarity_score(y_true, y_pred):
    """Calculates the average cosine similarity for regression evaluation."""
    if y_true.ndim == 1: y_true = y_true.reshape(-1, 1)
    if y_pred.ndim == 1: y_pred = y_pred.reshape(-1, 1)
    dot = np.sum(y_true * y_pred, axis=1)
    norms = norm(y_true, axis=1) * norm(y_pred, axis=1)
    return np.mean(dot / (norms + 1e-9))

def train_probe_and_eval(X, y, task_type, seed=42):
    """
    Trains and evaluates a linear probe on the provided data.
    It internally splits the data 70/30 for robust evaluation.
    """
    if y.ndim == 1: y = y.reshape(-1, 1)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)
    
    scaler = StandardScaler().fit(X_train)
    X_train_s, X_test_s = scaler.transform(X_train), scaler.transform(X_test)
    
    if task_type == 'classification':
        y_train, y_test = y_train.ravel(), y_test.ravel()
        probe = LogisticRegression(max_iter=1000, random_state=seed, n_jobs=-1).fit(X_train_s, y_train)
        return probe.score(X_test_s, y_test)
    else: # regression
        probe = Ridge(random_state=seed).fit(X_train_s, y_train)
        preds = probe.predict(X_test_s)
        return cosine_similarity_score(y_test, preds)

# =========================================================================
# 2. Main Script Logic
# =========================================================================

def main():
    parser = argparse.ArgumentParser(description="Run a probing analysis.")
    parser.add_argument("--dataset", choices=["narratives", "ud"], required=True, help="Dataset to probe.")
    parser.add_argument("--method", choices=["leace", "oracle"], required=True, help="Erasure method to probe.")
    args = parser.parse_args()

    # --- Setup ---
    method = args.method
    CONCEPTS = ["pos", "deplab", "sd"]
    dataset_name_short = "nar" if args.dataset == "narratives" else "ud"
    dataset_dir_name = "Narratives" if args.dataset == "narratives" else "UD"
    base_dir = "Final"

    print(f"\n\n{'='*80}")
    print(f"PROBING ANALYSIS for Method: {method.upper()}, Dataset: {args.dataset.upper()}")
    print(f"{'='*80}")
    
    raw_scores = pd.DataFrame(index=CONCEPTS, columns=["original"] + CONCEPTS, dtype=float)
    chance_scores = {}

    # --- Data Loading and Probing Loop ---
    for probed_concept in tqdm(CONCEPTS, desc="Probing Concept"):
        task_type = 'classification' if probed_concept in ['pos', 'deplab'] else 'regression'

        # --- Part A: Get the ground-truth labels (Z) and baseline data (X_original) ---
        if method == "leace":
            probe_data_path = f"{base_dir}/Embeddings/Erased/{dataset_dir_name}/leace_{dataset_name_short}_{probed_concept}_for_probing.pkl"
            try:
                with open(probe_data_path, "rb") as f: p_data = pickle.load(f)
            except FileNotFoundError:
                print(f"  - WARNING: Probing data file for '{probed_concept}' not found at {probe_data_path}. Skipping row.")
                continue
            
            X_original = p_data["X_test_original"]
            Z_labels = p_data["Z_test"]
            
            if task_type == 'classification':
                class_indices = np.argmax(Z_labels, axis=1)
                counts = Counter(class_indices); chance_scores[probed_concept] = counts.most_common(1)[0][1] / len(class_indices)
                Z_labels = class_indices
            else:
                chance_scores[probed_concept] = 0.0

        else: # ORACLE METHOD
            concept_key_map = {"pos": "pos_tags", "deplab": "deplabs", "sd": "distance_spectral_padded"}
            internal_key = concept_key_map[probed_concept]
            orig_path = f"{base_dir}/Embeddings/Original/{dataset_dir_name}/Embed_{dataset_name_short}_{internal_key}.pkl"
            try:
                with open(orig_path, "rb") as f: data = pickle.load(f)
            except FileNotFoundError:
                print(f"  - WARNING: Original data for '{probed_concept}' not found. Skipping row."); continue

            X_original = np.vstack([s["embeddings_by_layer"][8].numpy() for s in data])
            if task_type == 'classification':
                labels_flat = [lab for s in data for lab in s[internal_key]]
                counts = Counter(labels_flat); chance_scores[probed_concept] = counts.most_common(1)[0][1] / len(labels_flat)
                Z_labels = np.array(labels_flat)
            else:
                Z_labels = np.vstack([s[internal_key].cpu().numpy() for s in data])
                chance_scores[probed_concept] = 0.0
        
        # Calculate the baseline 'original' score
        raw_scores.loc[probed_concept, "original"] = train_probe_and_eval(X_original, Z_labels, task_type)

        # --- Part B: Probe the erased embeddings ---
        for erased_concept in CONCEPTS:
            if method == "leace":
                erased_path = f"{base_dir}/Embeddings/Erased/{dataset_dir_name}/leace_{dataset_name_short}_{erased_concept}_for_probing.pkl"
                try:
                    with open(erased_path, "rb") as f: e_data = pickle.load(f)
                    X_erased = e_data["X_test_erased"]
                except FileNotFoundError:
                    print(f"  - WARNING: Erased file for '{erased_concept}' not found. Skipping cell."); continue
            else: # oracle
                erased_path = f"{base_dir}/Embeddings/Erased/{dataset_dir_name}/oracle_{dataset_name_short}_{erased_concept}_vec.pkl"
                try:
                    with open(erased_path, "rb") as f: e_data = pickle.load(f)
                    X_erased = np.vstack(e_data['all_erased'])
                except FileNotFoundError:
                    print(f"  - WARNING: Erased file for '{erased_concept}' not found. Skipping cell."); continue
            
            # The key step: probe the erased X against the Z from the probed_concept
            raw_scores.loc[probed_concept, erased_concept] = train_probe_and_eval(X_erased, Z_labels, task_type)

    # --- Calculate RDI and Display/Save Results ---
    rdi_scores = pd.DataFrame(index=CONCEPTS, columns=CONCEPTS, dtype=float)
    for p_concept in CONCEPTS:
        p_before = raw_scores.loc[p_concept, "original"]
        if pd.isna(p_before): continue
        for e_concept in CONCEPTS:
            p_after = raw_scores.loc[p_concept, e_concept]
            if pd.isna(p_after): continue
            chance = chance_scores.get(p_concept, 0)
            if p_concept in ["pos", "deplab"]:
                denominator = p_before - chance
                rdi = 1.0 - (p_after - chance) / denominator if denominator > 1e-9 else 0.0
            else:
                denominator = p_before
                rdi = 1.0 - p_after / denominator if abs(denominator) > 1e-9 else 0.0
            rdi_scores.loc[p_concept, e_concept] = rdi

    pd.set_option('display.float_format', '{:.4f}'.format)
    print("\n\n" + "="*70)
    print(f"RESULTS for Method: {method.upper()}")
    print("-" * 70)
    print("\n--- Raw Probe Performance (Accuracy / Cosine Similarity) ---")
    print("Columns: Concept Erased From Embeddings | Rows: Concept Being Probed\n")
    print(raw_scores)
    print("\n--- RDI (Normalized Relative Drop in Information) Scores ---")
    print("1.0 = Full Erasure, 0.0 = No Erasure\n")
    print(rdi_scores)
    print("\n" + "="*80)

    # =====================================================================
    # === FINAL STEP: SAVE THE RESULTS TO CSV FILES ===
    # =====================================================================
    results_dir = f"Final/Results/{dataset_dir_name}"
    os.makedirs(results_dir, exist_ok=True)
    raw_scores.to_csv(f"{results_dir}/probing_raw_perf_{args.dataset}_{method}.csv")
    rdi_scores.to_csv(f"{results_dir}/probing_rdi_scores_{args.dataset}_{method}.csv")
    print(f"\nResults tables saved to CSV files in: {results_dir}")

if __name__ == "__main__":
    main()