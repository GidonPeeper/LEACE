"""
Unified script for probing original and erased GPT-2 embeddings.

This script evaluates the amount of information about various concepts that remains
in embeddings after erasure. It generates two 4x4 tables for a given dataset
and erasure method:
1. A raw performance matrix (Accuracy for discrete, R-squared for continuous).
2. A Remnant-to-Discarded Information (RDI) score matrix.

Probing Models:
- Logistic Regression for discrete concepts (pos, deplab).
- Ridge Regression for continuous concepts (sd, sdm).
"""
import argparse
import pickle
import numpy as np
import os
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, Ridge
from sklearn.metrics import accuracy_score, r2_score
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm

# =========================================================================
# 1. Data Loading and Preparation Helpers
# =========================================================================

def load_embeddings(file_path):
    """Loads embeddings from a .pkl file and flattens them into a single matrix."""
    try:
        with open(file_path, "rb") as f:
            data = pickle.load(f)
    except FileNotFoundError:
        print(f"ERROR: Input file not found at '{file_path}'")
        return None
    
    if "all_erased" in data:
        return np.vstack(data["all_erased"])
    else:
        # Layer 8 is hardcoded, matching the erasure script
        return np.vstack([sent["embeddings_by_layer"][8].numpy() for sent in data])

def prepare_concept_data(dataset, concept_cli):
    """Loads the original embeddings for a concept and prepares the Z matrix."""
    dataset_dir_name = "Narratives" if dataset == "narratives" else "UD"
    dataset_name_short = "nar" if dataset == "narratives" else "ud"

    # *** THIS MAP IS THE KEY CORRECTION ***
    # It maps the CLI concept name to BOTH the key inside the pickle file
    # AND the unique key used in the filename for original embeddings.
    concept_map = {
        "pos": {"internal_key": "pos_tags", "file_key": "pos"},
        "deplab": {"internal_key": "deplabs", "file_key": "deplab"},
        "sd": {"internal_key": "distance_spectral_padded", "file_key": "distance_spectral_padded"},
        "sdm": {"internal_key": "distance_matrix", "file_key": "distance_matrix"}
    }
    
    internal_key = concept_map[concept_cli]["internal_key"]
    file_key = concept_map[concept_cli]["file_key"]
    
    # Construct the correct path to the ORIGINAL embeddings file
    file_path = f"Final/Embeddings/Original/{dataset_dir_name}/Embed_{dataset_name_short}_{file_key}.pkl"
    
    try:
        with open(file_path, "rb") as f:
            data = pickle.load(f)
    except FileNotFoundError:
        print(f"ERROR: Could not find original data for concept '{concept_cli}' at '{file_path}'")
        return None, None, None
    
    # Prepare the Z matrix (the target labels/vectors)
    if concept_cli in ["pos", "deplab"]:
        all_labels_flat = [lab for sent in data for lab in sent.get(internal_key, [])]
        if not all_labels_flat: return None, None, None
        
        counts = Counter(all_labels_flat)
        chance = counts.most_common(1)[0][1] / len(all_labels_flat)
        Z = all_labels_flat
    
    elif concept_cli == "sd":
        Z = np.vstack([sent[internal_key].cpu().numpy() for sent in data])
        chance = 0.0

    elif concept_cli == "sdm":
        max_len = max(sent[internal_key].shape[0] for sent in data)
        all_z_vectors = []
        for sent in data:
            dist_matrix = sent[internal_key].numpy()
            sent_len = dist_matrix.shape[0]
            for i in range(sent_len):
                row_vector = dist_matrix[i, :]
                padded_vector = np.pad(row_vector, (0, max_len - sent_len), 'constant', constant_values=0)
                all_z_vectors.append(padded_vector)
        Z = np.vstack(all_z_vectors).astype(np.float32)
        chance = 0.0
    
    # Also load the corresponding X matrix (the original embeddings)
    X_orig = load_embeddings(file_path)

    return X_orig, Z, chance

# =========================================================================
# 2. Probing Function
# =========================================================================

def run_probe(X, Z, concept_type, seed=42):
    """Trains and evaluates a linear probe."""
    # Ensure Z is a numpy array for splitting if it's a list (for pos/deplab)
    if isinstance(Z, list):
        Z = np.array(Z)

    X_train, X_test, Z_train, Z_test = train_test_split(X, Z, test_size=0.3, random_state=seed)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    if concept_type in ["pos", "deplab"]:
        probe = LogisticRegression(max_iter=1000, random_state=seed, n_jobs=-1)
        probe.fit(X_train_scaled, Z_train)
        preds = probe.predict(X_test_scaled)
        return accuracy_score(Z_test, preds)
    elif concept_type in ["sd", "sdm"]:
        probe = Ridge(random_state=seed)
        probe.fit(X_train_scaled, Z_train)
        return r2_score(Z_test, probe.predict(X_test_scaled))

# =========================================================================
# 3. Main Script Logic
# =========================================================================

def main():
    parser = argparse.ArgumentParser(description="Run a probing analysis on erased embeddings.")
    parser.add_argument("--dataset", choices=["narratives", "ud"], required=True, help="Dataset to probe.")
    parser.add_argument("--method", choices=["leace", "oracle"], required=True, help="Erasure method to probe.")
    args = parser.parse_args()
    
    CONCEPTS = ["pos", "deplab", "sd", "sdm"]
    dataset_dir_name = "Narratives" if args.dataset == "narratives" else "UD"
    dataset_name_short = "nar" if args.dataset == "narratives" else "ud"
    
    raw_scores = pd.DataFrame(index=CONCEPTS, columns=CONCEPTS, dtype=float)
    rdi_scores = pd.DataFrame(index=CONCEPTS, columns=CONCEPTS, dtype=float)

    print(f"--- Starting Probing Analysis for Dataset: {args.dataset.upper()}, Method: {args.method.upper()} ---")

    # --- Step 1: Calculate baseline performance on ORIGINAL embeddings ---
    perf_before = {}
    chance_scores = {}
    print("\n[Phase 1/3] Calculating baseline performance on original embeddings...")
    for probed_concept in tqdm(CONCEPTS, desc="Probing original"):
        X_orig, Z, chance = prepare_concept_data(args.dataset, probed_concept)
        if X_orig is None or Z is None:
            print(f"Warning: Could not load data for concept {probed_concept}. Skipping.")
            continue
            
        perf_before[probed_concept] = run_probe(X_orig, Z, probed_concept)
        chance_scores[probed_concept] = chance

    # --- Step 2: Calculate performance on ERASED embeddings ---
    print("\n[Phase 2/3] Calculating performance on erased embeddings...")
    pbar = tqdm(total=len(CONCEPTS)**2, desc="Probing erased")
    for erased_concept in CONCEPTS: # Columns
        emb_path = f"Final/Embeddings/Erased/{dataset_dir_name}/{args.method}_{dataset_name_short}_{erased_concept}_vec.pkl"
        X_erased = load_embeddings(emb_path)
        if X_erased is None:
            # Skip the whole column if the erased file doesn't exist
            for probed_concept in CONCEPTS:
                raw_scores.loc[probed_concept, erased_concept] = np.nan
                pbar.update(1)
            continue
        
        for probed_concept in CONCEPTS: # Rows
            pbar.set_description(f"Probing {probed_concept} from {erased_concept}-erased")
            
            # We only need the Z data now
            _, Z, _ = prepare_concept_data(args.dataset, probed_concept)
            if Z is None:
                raw_scores.loc[probed_concept, erased_concept] = np.nan
                pbar.update(1)
                continue
            
            perf_after = run_probe(X_erased, Z, probed_concept)
            raw_scores.loc[probed_concept, erased_concept] = perf_after
            pbar.update(1)
    pbar.close()

    # --- Step 3: Calculate RDI scores and Display Results ---
    print("\n[Phase 3/3] Calculating RDI scores...")
    for probed_concept in CONCEPTS:
        for erased_concept in CONCEPTS:
            if pd.isna(raw_scores.loc[probed_concept, erased_concept]):
                continue
                
            p_after = raw_scores.loc[probed_concept, erased_concept]
            p_before = perf_before.get(probed_concept, 0)
            chance = chance_scores.get(probed_concept, 0)
            
            if probed_concept in ["pos", "deplab"]:
                denominator = p_before - chance
                rdi = 1.0 - (p_after - chance) / denominator if denominator > 1e-6 else (1.0 if p_after <= p_before else 0.0)
            else: # sd, sdm
                denominator = p_before
                rdi = 1.0 - p_after / denominator if denominator > 1e-6 else (1.0 if p_after <= p_before else 0.0)
            
            rdi_scores.loc[probed_concept, erased_concept] = max(0, rdi)

    # --- Display Results ---
    pd.set_option('display.float_format', '{:.3f}'.format)
    print("\n\n" + "="*80)
    print(f"RESULTS for Dataset: {args.dataset.upper()}, Erasure Method: {args.method.upper()}")
    print("="*80)
    
    print("\n--- Raw Probe Performance (Accuracy / R-squared) ---")
    print("Columns: Concept Erased From Embeddings")
    print("Rows: Concept Being Probed\n")
    print(raw_scores)
    
    print("\n\n--- RDI (Remnant-to-Discarded Information) Scores ---")
    print("1.0 = Full Erasure, 0.0 = No Erasure\n")
    print(rdi_scores)
    print("\n" + "="*80)
    
    results_dir = f"Final/Results/{dataset_dir_name}"
    raw_scores.to_csv(f"{results_dir}/probing_raw_perf_{args.dataset}_{args.method}.csv")
    rdi_scores.to_csv(f"{results_dir}/probing_rdi_scores_{args.dataset}_{args.method}.csv")
    print(f"\nResults saved to {results_dir}")

if __name__ == "__main__":
    main()